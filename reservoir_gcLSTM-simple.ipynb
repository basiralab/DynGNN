{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24289,
     "status": "ok",
     "timestamp": 1694168463049,
     "user": {
      "displayName": "Павел Бозмаров",
      "userId": "13372247782600851572"
     },
     "user_tz": -60
    },
    "id": "7q88jl49dAZx",
    "outputId": "87b07f64-7289-4e79-b63b-cc5759d44b85"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "import argparse\n",
    "import timeit\n",
    "from dataset import prepare_data\n",
    "import psutil\n",
    "import torch\n",
    "from models.gcLSTM_simple import GCLSTMModel\n",
    "from memory_capacity_utils import gen_lag_data, compute_memory_capacity_vectorized, get_mem_cap_from_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('running on GPU')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuyWYNaidZie"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Args for graph predition')\n",
    "    parser.add_argument('-num_folds', type=int, default=3, help='cv number')\n",
    "    parser.add_argument('--num_timepoints', type=int, default=3,\n",
    "                        help='Number of timepoints')\n",
    "    parser.add_argument('-num_epochs', type=int, default=30, help='number of epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help=\"Learning rate\")\n",
    "    parser.add_argument('--memcap_coef', type=float, default=0.001, help=\"Memory Capacity Loss Coefficient\")\n",
    "    parser.add_argument('-max_lag', type=int, default=35, help='Lag tao for memory capacity signals')\n",
    "    parser.add_argument('-save_path',type=str,default = '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/',help='Path to the saved results')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1579,
     "status": "ok",
     "timestamp": 1694168465038,
     "user": {
      "displayName": "Павел Бозмаров",
      "userId": "13372247782600851572"
     },
     "user_tz": -60
    },
    "id": "CJfqLrF4bbv1"
   },
   "outputs": [],
   "source": [
    "dataset = np.load('datasets/oasis_data.npy')\n",
    "#dataset = np.load(\"datasets/multivariate_simulation_data_2.npy\")\n",
    "dataset = torch.from_numpy(dataset).squeeze()\n",
    "dataset = dataset.type(torch.FloatTensor)\n",
    "dataset = np.delete(dataset,88,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([113, 3, 35, 35])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq-tRUzrK_Zw"
   },
   "source": [
    "# Set up the network architecture and train-validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljZ0c4g_uu9g"
   },
   "source": [
    "**Train-validate-functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/real_and_predicted_graphs' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/train_losses/mae_losses' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/train_losses/reservoir_losses' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/train_losses/bio_losses' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/train_losses/tp_losses' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/train_losses/total_losses' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/test_mae_losses' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/test_tp_losses' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/test_memcap_losses' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/test_predicted' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/test_original' already exists.\n",
      "Directory '/vol/bitbucket/sx420/4D-FedGNN-Plus/results/reservoir_gcLSTM/trained_models' already exists.\n"
     ]
    }
   ],
   "source": [
    "def create_directory_if_not_exists(directory):\n",
    "    \"\"\"\n",
    "    Checks if a specified directory exists, and creates it if it doesn't.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): Path of the directory to check and potentially create.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' was created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "# Create the results folders\n",
    "create_directory_if_not_exists(args.save_path+'real_and_predicted_graphs')\n",
    "create_directory_if_not_exists(args.save_path+'train_losses/mae_losses')\n",
    "create_directory_if_not_exists(args.save_path+'train_losses/reservoir_losses')\n",
    "create_directory_if_not_exists(args.save_path+'train_losses/bio_losses')\n",
    "create_directory_if_not_exists(args.save_path+'train_losses/tp_losses')\n",
    "create_directory_if_not_exists(args.save_path+'train_losses/total_losses')\n",
    "create_directory_if_not_exists(args.save_path+'test_mae_losses')\n",
    "create_directory_if_not_exists(args.save_path+'test_tp_losses')\n",
    "create_directory_if_not_exists(args.save_path+'test_memcap_losses')\n",
    "create_directory_if_not_exists(args.save_path+'test_predicted')\n",
    "create_directory_if_not_exists(args.save_path+'test_original')\n",
    "create_directory_if_not_exists(args.save_path+'trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed = 777\n",
    "np.random.seed(manual_seed)\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "# Reservoir signals\n",
    "X_train_res_np, y_train_res_np = gen_lag_data(1000, 41, args.max_lag)\n",
    "X_test_res_np, y_test_res_np = gen_lag_data(500, 42, args.max_lag)\n",
    "X_train_res = torch.from_numpy(X_train_res_np).unsqueeze(1).to(device, dtype=torch.float64)\n",
    "X_test_res = torch.from_numpy(X_test_res_np).unsqueeze(1).to(device, dtype=torch.float64)\n",
    "y_train_res = torch.from_numpy(y_train_res_np).to(device, dtype=torch.float64)\n",
    "y_test_res = torch.from_numpy(y_test_res_np).to(device, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, validation_subjects, mem_cap_data, X_train, y_train, X_test, y_test):\n",
    "    mael = torch.nn.L1Loss().to(device)\n",
    "    tp = torch.nn.MSELoss().to(device)\n",
    "    \n",
    "    val_mae_loss = np.zeros(args.num_timepoints - 1)\n",
    "    val_tp_loss = np.zeros(args.num_timepoints - 1)\n",
    "    mem_cap = np.zeros(args.num_timepoints - 1)\n",
    "    predicted = np.zeros((validation_subjects.shape[0], args.num_timepoints - 1, 35, 35))\n",
    "    actual = np.zeros((validation_subjects.shape[0], args.num_timepoints - 1, 35, 35))\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for n_subject, data in enumerate(validation_subjects):\n",
    "            input = data[0]\n",
    "            for t in range(args.num_timepoints - 1):\n",
    "                pred = model(input)\n",
    "                val_mae_loss[t] += mael(pred, data[t + 1])\n",
    "                val_tp_loss[t] += tp(pred.sum(dim=-1), data[t + 1].sum(dim=-1))\n",
    "                input = pred\n",
    "                \n",
    "                pred_mem_cap = get_mem_cap_from_model(model, pred, \n",
    "                                                      X_train, y_train, X_test, y_test)\n",
    "                actual_mem_cap = torch.tensor(mem_cap_data[n_subject, t + 1]).to(device)\n",
    "                mem_cap[t] += torch.abs(pred_mem_cap - actual_mem_cap)\n",
    "\n",
    "                predicted[n_subject, t] = pred.cpu().detach().numpy()\n",
    "                actual[n_subject, t] = data[t + 1].cpu().detach().numpy()\n",
    "                \n",
    "\n",
    "    avg_val_mae_loss = val_mae_loss/len(validation_subjects)\n",
    "    avg_val_tp_loss = val_tp_loss/len(validation_subjects)\n",
    "    avg_val_mae_mem_cap = mem_cap/len(validation_subjects)\n",
    "\n",
    "    return avg_val_mae_loss, avg_val_tp_loss, avg_val_mae_mem_cap, predicted, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------Fold [1/3]-----------------------------------------\n",
      "Epoch [1/30]\n",
      "[Train] MAE Loss: 0.08611517579605182, TP Loss: 6.807798506816228, MAE of Mem Caps Loss: 0.46594317731112816\n",
      "[Validate] MAE Loss Across Timepoints: [0.08465891 0.06932971]\n",
      "[Validate] TP Loss Across Timepoints: [13.12434184  3.74944127]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.87411208 1.65864365]\n",
      "Epoch [2/30]\n",
      "[Train] MAE Loss: 0.07427550299714009, TP Loss: 5.916689925392469, MAE of Mem Caps Loss: 0.34504988820824767\n",
      "[Validate] MAE Loss Across Timepoints: [0.07620006 0.06201723]\n",
      "[Validate] TP Loss Across Timepoints: [11.62934469  2.97289276]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.97577594 1.60756261]\n",
      "Epoch [3/30]\n",
      "[Train] MAE Loss: 0.06679011198381583, TP Loss: 4.917066740989685, MAE of Mem Caps Loss: 0.33989585940150246\n",
      "[Validate] MAE Loss Across Timepoints: [0.07006126 0.05725748]\n",
      "[Validate] TP Loss Across Timepoints: [10.51773682  2.45515518]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.79109855 1.5861129 ]\n",
      "Epoch [4/30]\n",
      "[Train] MAE Loss: 0.0638752923036615, TP Loss: 4.6181048611799875, MAE of Mem Caps Loss: 0.3214505356428131\n",
      "[Validate] MAE Loss Across Timepoints: [0.06845172 0.05559457]\n",
      "[Validate] TP Loss Across Timepoints: [10.27353312  2.39077123]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.68117567 1.48122903]\n",
      "Epoch [5/30]\n",
      "[Train] MAE Loss: 0.06183752616246541, TP Loss: 4.330180779099464, MAE of Mem Caps Loss: 0.31806295064645557\n",
      "[Validate] MAE Loss Across Timepoints: [0.06685859 0.05395625]\n",
      "[Validate] TP Loss Across Timepoints: [9.96234945 2.22888819]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.61291181 1.18402649]\n",
      "Epoch [6/30]\n",
      "[Train] MAE Loss: 0.06133713355908791, TP Loss: 4.354380546510219, MAE of Mem Caps Loss: 0.33628917707463374\n",
      "[Validate] MAE Loss Across Timepoints: [0.06646723 0.0535132 ]\n",
      "[Validate] TP Loss Across Timepoints: [9.92794189 2.24850769]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.58654604 0.89981208]\n",
      "Epoch [7/30]\n",
      "[Train] MAE Loss: 0.059592003499468166, TP Loss: 4.03624103864034, MAE of Mem Caps Loss: 0.3017781461279059\n",
      "[Validate] MAE Loss Across Timepoints: [0.06414661 0.05116748]\n",
      "[Validate] TP Loss Across Timepoints: [9.30386861 1.85006205]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.59832313 0.81407071]\n",
      "Epoch [8/30]\n",
      "[Train] MAE Loss: 0.05859856971849998, TP Loss: 3.9597491577267645, MAE of Mem Caps Loss: 0.32616489192668124\n",
      "[Validate] MAE Loss Across Timepoints: [0.06398955 0.05060316]\n",
      "[Validate] TP Loss Across Timepoints: [9.43032328 1.88352076]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.59542392 0.7445086 ]\n",
      "Epoch [9/30]\n",
      "[Train] MAE Loss: 0.05790655246625344, TP Loss: 3.8828515966733295, MAE of Mem Caps Loss: 0.29910228127597455\n",
      "[Validate] MAE Loss Across Timepoints: [0.06358972 0.05004886]\n",
      "[Validate] TP Loss Across Timepoints: [9.22370199 1.68893738]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.58381499 0.78707293]\n",
      "Epoch [10/30]\n",
      "[Train] MAE Loss: 0.0581558241819342, TP Loss: 3.980112435420354, MAE of Mem Caps Loss: 0.2931764344535926\n",
      "[Validate] MAE Loss Across Timepoints: [0.06374534 0.05007905]\n",
      "[Validate] TP Loss Across Timepoints: [9.42795003 1.83772952]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.59403707 0.72768941]\n",
      "Epoch [11/30]\n",
      "[Train] MAE Loss: 0.05772435733427604, TP Loss: 3.9325935835639636, MAE of Mem Caps Loss: 0.30440845578823217\n",
      "[Validate] MAE Loss Across Timepoints: [0.06357011 0.04961818]\n",
      "[Validate] TP Loss Across Timepoints: [9.325354   1.72051442]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.58357081 0.73286723]\n",
      "Epoch [12/30]\n",
      "[Train] MAE Loss: 0.057686673601468404, TP Loss: 3.933244362970193, MAE of Mem Caps Loss: 0.2861573352880954\n",
      "[Validate] MAE Loss Across Timepoints: [0.06348745 0.04946233]\n",
      "[Validate] TP Loss Across Timepoints: [9.26153666 1.64415538]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.58530235 0.73424197]\n",
      "Epoch [13/30]\n",
      "[Train] MAE Loss: 0.05662323360641797, TP Loss: 3.8231133873263996, MAE of Mem Caps Loss: 0.3147035799791505\n",
      "[Validate] MAE Loss Across Timepoints: [0.06157424 0.04744228]\n",
      "[Validate] TP Loss Across Timepoints: [9.18882345 1.62994194]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.55839641 0.73039176]\n",
      "Epoch [14/30]\n",
      "[Train] MAE Loss: 0.055395741512378055, TP Loss: 3.7351141701141994, MAE of Mem Caps Loss: 0.32314823839188467\n",
      "[Validate] MAE Loss Across Timepoints: [0.06112231 0.04710602]\n",
      "[Validate] TP Loss Across Timepoints: [8.81027018 1.33733788]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.57604857 0.73507534]\n",
      "Epoch [15/30]\n",
      "[Train] MAE Loss: 0.05528337837507327, TP Loss: 3.7190814817945164, MAE of Mem Caps Loss: 0.32378672022437816\n",
      "[Validate] MAE Loss Across Timepoints: [0.06105224 0.04695133]\n",
      "[Validate] TP Loss Across Timepoints: [8.82957458 1.36298409]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.55179192 0.71831697]\n",
      "Epoch [16/30]\n",
      "[Train] MAE Loss: 0.05538284328455726, TP Loss: 3.717732349038124, MAE of Mem Caps Loss: 0.36162003222831995\n",
      "[Validate] MAE Loss Across Timepoints: [0.06124798 0.04710188]\n",
      "[Validate] TP Loss Across Timepoints: [8.97040405 1.41742846]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.55371023 0.73331594]\n",
      "Epoch [17/30]\n",
      "[Train] MAE Loss: 0.05506157027557492, TP Loss: 3.631581853330135, MAE of Mem Caps Loss: 0.32197294362036283\n",
      "[Validate] MAE Loss Across Timepoints: [0.06120554 0.04717966]\n",
      "[Validate] TP Loss Across Timepoints: [8.95762227 1.39464353]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.55618443 0.73603967]\n",
      "Epoch [18/30]\n",
      "[Train] MAE Loss: 0.0550813637363414, TP Loss: 3.6995979607105256, MAE of Mem Caps Loss: 0.3175814352865937\n",
      "[Validate] MAE Loss Across Timepoints: [0.06111361 0.04697127]\n",
      "[Validate] TP Loss Across Timepoints: [8.89931234 1.33985634]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.56702723 0.73030579]\n",
      "Epoch [19/30]\n",
      "[Train] MAE Loss: 0.05487299207597971, TP Loss: 3.610282230377197, MAE of Mem Caps Loss: 0.3130969309682616\n",
      "[Validate] MAE Loss Across Timepoints: [0.06106583 0.04714977]\n",
      "[Validate] TP Loss Across Timepoints: [8.84856669 1.30347303]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.52199767 0.7389224 ]\n",
      "Epoch [20/30]\n",
      "[Train] MAE Loss: 0.05515652019530535, TP Loss: 3.670763112604618, MAE of Mem Caps Loss: 0.30549652255313203\n",
      "[Validate] MAE Loss Across Timepoints: [0.06102704 0.04676885]\n",
      "[Validate] TP Loss Across Timepoints: [8.88838704 1.33964437]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.52776941 0.70837886]\n",
      "Epoch [21/30]\n",
      "[Train] MAE Loss: 0.05474913793926438, TP Loss: 3.6531718303759892, MAE of Mem Caps Loss: 0.3026762384879109\n",
      "[Validate] MAE Loss Across Timepoints: [0.06108562 0.04676296]\n",
      "[Validate] TP Loss Across Timepoints: [8.95971578 1.37944489]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.54184547 0.71789899]\n",
      "Epoch [22/30]\n",
      "[Train] MAE Loss: 0.05501644778996706, TP Loss: 3.7100077589352924, MAE of Mem Caps Loss: 0.29150141291883475\n",
      "[Validate] MAE Loss Across Timepoints: [0.06106633 0.04673338]\n",
      "[Validate] TP Loss Across Timepoints: [8.93058675 1.35481135]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.5326331  0.71763801]\n",
      "Epoch [23/30]\n",
      "[Train] MAE Loss: 0.054654254702230294, TP Loss: 3.6267105862498283, MAE of Mem Caps Loss: 0.3233005668725423\n",
      "[Validate] MAE Loss Across Timepoints: [0.06106236 0.04657321]\n",
      "[Validate] TP Loss Across Timepoints: [8.98390808 1.39328245]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.51154265 0.71076649]\n",
      "Epoch [24/30]\n",
      "[Train] MAE Loss: 0.05490472860013445, TP Loss: 3.6946675871809322, MAE of Mem Caps Loss: 0.3246292671856786\n",
      "[Validate] MAE Loss Across Timepoints: [0.06111153 0.04672253]\n",
      "[Validate] TP Loss Across Timepoints: [8.98103638 1.37163709]\n",
      "[Validate] MAE of Mem Caps Across Timepoints: [0.53657744 0.71047107]\n",
      "Epoch [25/30]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "input_weights = (torch.rand((35, 1), dtype=torch.float64) * 2.0 - 1.0).to(device)\n",
    "\n",
    "indexes = range(args.num_folds)\n",
    "kfold = KFold(n_splits=args.num_folds, shuffle=True, random_state=manual_seed)\n",
    "dataset = dataset.to(device)\n",
    "actual_mem_caps = np.load('datasets/mem_caps_oasis.npy')\n",
    "f = 0\n",
    "\n",
    "for train, test in kfold.split(range(dataset.shape[0])):\n",
    "    print(\n",
    "            f'------------------------------------Fold [{f + 1}/{args.num_folds}]-----------------------------------------')\n",
    "    \n",
    "    train_data = dataset[train]\n",
    "    test_data = dataset[test]\n",
    "    train_mem_cap = actual_mem_caps[train]\n",
    "    test_mem_cap = actual_mem_caps[test]\n",
    "    \n",
    "    validation_split = int(0.8 * len(train_data))\n",
    "    train_subjects = train_data[:validation_split]\n",
    "    train_mem_cap_subjects = train_mem_cap[:validation_split]\n",
    "    validation_subjects = train_data[validation_split:]\n",
    "    validation_mem_cap_subjects = train_mem_cap[:validation_split]\n",
    "\n",
    "    model = GCLSTMModel(device=device, input_weights=input_weights).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    mael = torch.nn.L1Loss().to(device)\n",
    "    tp = torch.nn.MSELoss().to(device)\n",
    "  \n",
    "    # Start measuring the epochs time\n",
    "    epochs_start = time.time()\n",
    "    for epoch in range(args.num_epochs):\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{args.num_epochs}]')\n",
    "        # Set the model in training mode\n",
    "        model.train()\n",
    "        \n",
    "        # this is our loss for all the data\n",
    "        mae_loss_overall = []\n",
    "        tp_loss_overall = []\n",
    "        mae_mem_cap_overall = []\n",
    "        \n",
    "        # loop through the data batches\n",
    "        for data_id, data in enumerate(train_subjects):\n",
    "\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            mae_loss = 0\n",
    "            tp_loss = 0\n",
    "            mem_cap_loss = 0\n",
    "         \n",
    "            # loop through the time dependent adj matrices in the batches\n",
    "            for t in range(args.num_timepoints - 1):\n",
    "                pred, output_sig = model(data[t], X_train_res, y_train_res, X_test_res)\n",
    "                    \n",
    "                real = data[t + 1]\n",
    "                \n",
    "                mae_loss += mael(pred, real)\n",
    "\n",
    "                # Topological Loss\n",
    "                tp_loss += tp(pred.sum(dim=-1), real.sum(dim=-1))\n",
    "\n",
    "                # MAE between predicted graph's mem cap and actual graph's mem cap\n",
    "                predicted_mem_cap = compute_memory_capacity_vectorized(output_sig, y_test_res)\n",
    "                actual_mem_cap = torch.tensor(train_mem_cap_subjects[data_id, t + 1], requires_grad=True).to(device)\n",
    "                mem_cap_loss += mael(predicted_mem_cap, actual_mem_cap)\n",
    "\n",
    "    \n",
    "            # Calculate the total MAE Loss for the current batch\n",
    "            mae_loss = mae_loss / (args.num_timepoints - 1)\n",
    "\n",
    "            # Calculate the total TP Loss for the current batch\n",
    "            tp_loss = tp_loss / (args.num_timepoints - 1)\n",
    "\n",
    "            # Calculate the total MAE between Mem Cap Loss for the current batch\n",
    "            mem_cap_loss = mem_cap_loss / (args.num_timepoints - 1)\n",
    "            \n",
    "            # Append to the total MAE Loss\n",
    "            mae_loss_overall.append(mae_loss.item())\n",
    "            tp_loss_overall.append(tp_loss.item())\n",
    "            mae_mem_cap_overall.append(mem_cap_loss.item())\n",
    "            \n",
    "            total_loss = mae_loss + args.memcap_coef * mem_cap_loss \n",
    "            # Update the weights of the neural network\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        mae_loss_overall = np.mean(np.array(mae_loss_overall))\n",
    "        tp_loss_overall = np.mean(np.array(tp_loss_overall))\n",
    "        mae_mem_cap_overall = np.mean(np.array(mae_mem_cap_overall))\n",
    "        print(f\"[Train] MAE Loss: {mae_loss_overall}, TP Loss: {tp_loss_overall}, MAE of Mem Caps Loss: {mae_mem_cap_overall}\")\n",
    "    \n",
    "        avg_val_mae_loss, avg_val_tp_loss, avg_val_mae_mem_cap, _, _ = validation(model, validation_subjects, validation_mem_cap_subjects, \n",
    "                                                                                  X_train_res, y_train_res, X_test_res, y_test_res)\n",
    "        print(f\"[Validate] MAE Loss Across Timepoints: {avg_val_mae_loss}\")\n",
    "        print(f\"[Validate] TP Loss Across Timepoints: {avg_val_tp_loss}\")\n",
    "        print(f\"[Validate] MAE of Mem Caps Across Timepoints: {avg_val_mae_mem_cap}\")\n",
    "\n",
    "    \n",
    "    epochs_end = time.time() - epochs_start\n",
    "    print()\n",
    "    print(f'epochs finished with time:{epochs_end}')\n",
    "    print()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"Current memory usage: {process.memory_info().rss / 1024 ** 2:.2f} MB\")\n",
    "    print()\n",
    "    \n",
    "    avg_test_mae_loss, avg_test_tp_loss, avg_test_mem_cap, predicted, original = validation(model, test_data, test_mem_cap, \n",
    "                                                                                            X_train_res, y_train_res, X_test_res, y_test_res)\n",
    "    print(f\"[Test] MAE Loss Across Timepoints: {avg_test_mae_loss}\")\n",
    "    print(f\"[Test] TP Loss Across Timepoints: {avg_test_tp_loss}\")\n",
    "    print(f\"[Test] MAE of Mem Caps Across Timepoints: {avg_test_mem_cap}\")\n",
    "    np.save(args.save_path+f\"test_mae_losses/mae_test_loss_fold_{f}\", avg_test_mae_loss)\n",
    "    np.save(args.save_path+f\"test_tp_losses/tp_test_loss_fold_{f}\", avg_test_mae_loss)\n",
    "    np.save(args.save_path+f\"test_memcap_losses/memcap_test_loss_fold_{f}\", avg_test_mem_cap)\n",
    "    np.save(args.save_path+f\"test_predicted/predicted_fold_{f}\", predicted)\n",
    "    np.save(args.save_path+f\"test_original/original_fold_{f}\", original)\n",
    "\n",
    "    torch.save(model.state_dict(),\n",
    "               args.save_path +f'trained_models/model_fold_{f}')\n",
    "    f += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "90ooXFhVd5Ci",
    "v7zuUNtIu26b",
    "boAUzmyY1b45",
    "gCeqTVgMstrm",
    "VoaPdwAI_3OO",
    "cBsklrSsPUtm",
    "ke3Z16hUbTSE",
    "E77zvOJPlTul",
    "tY7gZyeQaHdO",
    "BDKGEEKpqE9-",
    "8KHFUzhADA6_",
    "2kUqdyWRj5qF",
    "tHprtfbUku3t",
    "5DL6MoT5xRkX"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f5c6f72bc62c81ae52e96e9a3a4236b77333ef45d4cdc0c3574ebd317f415f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
